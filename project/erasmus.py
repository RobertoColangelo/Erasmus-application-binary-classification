# -*- coding: utf-8 -*-
"""Erasmus.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/111I1fcBR_ATDS-3h6Uk6Iq2j-FJSgZ23

##Importing Libraries
"""

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import plotly.express as px
import sklearn
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
from sklearn.svm import SVC
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.patches as patches
import seaborn as sns
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
!pip install openpyxl==3.0.9

"""##Dataframe creation"""

df = pd.read_excel('Data Science In Action Unified Survey (Risposte).xlsx')
df

"""##Selecting the attributes needed """

#Selecting the columns that seem to be more interesting for our purposes 
df1 = df.iloc[:,[1, 7, 8, 9, 12, 16, 18, 25, 29, 30, 32, 33, 36,40,48,54,55,56,57,60,74,77,83,84,86,92,110,126,136]]
#Renaming the columns to facilitate coding and  comprehension of the attributes 
df1 = df1.rename({'How would you describe your gender?': 'gender', "How many master's exams have you already passed?": 'exams_taken','How old are you?':'age','Do you have a scholarship?':'scholarship', 'After graduating, do you plan to work or to continue with your studies?':'future_plan','After your graduation, do you plan to stay in Italy or go abroad?':'leaving','Will you apply for an Erasmus program?':'Erasmus','How stressed do you feel on a scale from 1 (no stressed) to 5 (highly stressed)?':'levels_of_stress','How would you describe your dedication to your studies?':'study_dedication','How many languages do you speak fluently?':'number_of_languages','Where do you see yourself in 2 years?':'prevision_2_years_from_now','How many brothers or sisters do you have?':'n_siblings','Do you live with your family?': 'lives_with_family' ,'Are you currently in a relationship? (long distance or not)':'in_a_relationship','Do you consider yourself kind of bored?':'bored','Do you possess a car?':'has_a_car','What is the current mean of your grades?':'current_grades_mean', "What's your level of weekly alcohol assumption?":'weekly_level_of_alcohol', 'Do you prefer to study on your own or with some friends?':'studying_preferences','Where are you from? (specify country and city)':'origin','Can you manage private life and study?':'able_to_manage_life_and_study','Does the future scare you?':'scared_of_future','How many members are there in your family?':'family_members','What is your high school grade? Are you happy with it?':'high_school_grade','Do you feel happy?':'is_happy','In near future, would you rather to be an employee or an entrepreneur?':'wants_to_be','Have you had Erasmus experiences abroad? If so, where?':'has_had_Erasmus',"What was the mean of your grades in your bachelor's degree?":'bachelor_mean_grades','How many hours do you spend studying per day?':'hours_of_study_per_day'}, axis=1)  # new method
df1

"""##Data Visualization"""

#histogram of number of exams taken so far 
plt.style.use('ggplot')
bin_edges = np.arange(1.5, 5.5, 1)
plt.hist(df1.iloc[:,1], bins = bin_edges,edgecolor='black',color='green')
plt.xticks(np.arange(2, 4.5, 1))
plt.xlabel('Exams taken')
plt.show()

#Pie chart representing the sex percentage of students 
plt.style.use('default')
Pgender=df1.groupby(by=["gender"]).size()
colors=['r','b']
plt.pie(Pgender,colors=colors,labels=['Ladies','Boys'],autopct='%1.1f%%', shadow=False)
plt.title('Percentage of students by gender')
plt.show()

#Distribution of the current grades of students 
plt.style.use('classic')
df1['current_grades_mean'].plot.box(title='Distribution of current grades')
#It appears that the distribution of grades is very skewed towards high grades (the data might be biased since only good performing students are the ones doing the bonus project )

#Distribution of grades by gender 
plt.style.use('default')
df1.pivot(columns='gender', values='current_grades_mean').plot.hist()
plt.show()

#Fixing the instances in the 'origin' column since the format is not uniform
for el in range(0,len(df1['origin'])):
  df1['origin'][el]=df1['origin'][el].lower()
  if  'rome' in df1['origin'][el]:
    df1['origin'][el]='Rome'
  elif 'roma' in df1['origin'][el]:
    df1['origin'][el]='Rome'
  elif 'italy' in df1['origin'][el]:
    df1['origin'][el]='Italy'
  else:
    df1['origin'][el]='Abroad'

#piechart Origin
Origin=df1.groupby(by=["origin"]).size()
colors=['pink','yellow','purple']
labels=['Abroad','Rome','Italy']
plt.pie(Origin,colors=colors,labels=labels, autopct='%1.1f%%', shadow=False)
plt.title('Origin of students in percentages')
plt.show()

#catplot of family members vs levels of stress by gender 
sns.catplot(x="family_members", y="levels_of_stress", hue="gender", aspect=2,
            kind="swarm", data=df1)

#students whose families are larger seem to be more inclined to stress out with respect to others

"""#Data Manipulation and Pre-processing"""

#fixing has_had_Erasmus column 
for el in range(0,len(df1['has_had_Erasmus'])):
  df1['has_had_Erasmus'][el]=df1['has_had_Erasmus'][el].lower()
  if  'no' in df1['has_had_Erasmus'][el]:
    df1['has_had_Erasmus'][el]=0
  elif 'not' in df1['has_had_Erasmus'][el]:
    df1['has_had_Erasmus'][el]=0
  elif 'never'  in df1['has_had_Erasmus'][el]:
    df1['has_had_Erasmus'][el]=0
  elif "haven't" in df1['has_had_Erasmus'][el]:
    df1['has_had_Erasmus'][el]=0
  else:
    df1['has_had_Erasmus'][el]=1

#changing the format of the instances of the 'high_school_grade' column 
df1['high_school_grade'] = df1['high_school_grade'].str.extract('(\d+)', expand=False)
noNull=df1['high_school_grade'].dropna()
l=[]
for el in noNull:
  el=int(el)
  l.append(el)
avg_grade=sum(l)/len(l)
avg_grade
##this is the value will use to substitute the null value. We cannot just delete the instance because the number of instances is ridiculous
df1['high_school_grade'].fillna(round(avg_grade,0), inplace=True)
df1['high_school_grade']

#changing the format of the instances of the 'bachelor_mean_grades' column 
# Unfortunately our colleagues decided to play around and instead of typing the grade wrote stupid jokes so we had to fix some instances 
l2=[]
df1['bachelor_mean_grades'][15, 18] = 0 
l2=[]
for el in df1['bachelor_mean_grades']:
  if el != 0:
    el=float(el)
    l2.append(el)
l2
avg_mean=sum(l2)/len(l2)
avg_mean
##this is the value will use to substitute the null value. We cannot just delete the instance because the number of instances is ridiculous
df1['bachelor_mean_grades'][15, 18] = round(avg_mean,2)  
df1['bachelor_mean_grades']

#Preprocessing the data and transforming categorical columns with ordinal and one-hot encoding 
df1=df1.replace({"gender": {"Male":1, "Female":0},
                 "future_plan": {"Working":1, "Studying":0},
                 "leaving": {"Staying in Italy":1, "Going abroad":0},
                 "study_dedication": {"I speedrun tasks when time is over":1, "I just study whenever I feel guilty":2, "I have a plan for everyday":3},
                 "prevision_2_years_from_now": {"Still studying":1, "Looking for a job":2, "With a job that satisfy me":3},
                 "studying_preferences": {"On my own":1, "With friends":0},
                 "n_siblings": {"I am only child":0, "more than 2":3},
                 "is_happy": {"I don't know":2},
                 "wants_to_be": {"Employee":1, "I don't know":2, "Entrepreneur":3},
                 "number_of_languages": {"more than 3":4},
                 "origin": {"Rome":0, "Italy":1, "Abroad":2}})
df1.replace(('Yes', 'No'), (1, 0), inplace=True)
df1

"""##Creating target vector and matrix of predictors, Standardization"""

#Separating dependent and independent variables
y = df1['Erasmus'].values
x = df1.drop(df1.columns[[6]], axis=1)

#Standardizing the predictors 
scaler = StandardScaler()
scaler.fit(x)
x = scaler.transform(x)

#Creating a list that will contain the models' accuracies for the final comparison
results=[]

#Baseline model
log_regr = LogisticRegression()
y_pred = cross_val_predict(log_regr, x, y, cv=5)
conf_mat = confusion_matrix(y, y_pred)
print(conf_mat) 
acc=(conf_mat[0,0]+conf_mat[1,1])/(conf_mat[0,0]+conf_mat[1,1]+conf_mat[0,1]+conf_mat[1,0])
print("Mean 5-Fold accuracy of the logistic regression is: {}".format(acc))
results.append(acc)

"""##Dimension reduction """

#PCA
pca = PCA(n_components=10) #dal grafico sotto, spiega 96% varianza
pca.fit(x)
x = pca.transform(x)

exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)
px.area(
    x=range(1, exp_var_cumul.shape[0] + 1),
    y=exp_var_cumul,
    labels={"x": "# Components", "y": "Explained Variance"}
)

"""##Models with PCA"""

#Logistic regression pca
log_regr = LogisticRegression()
y_pred = cross_val_predict(log_regr, x, y, cv=5)
conf_mat = confusion_matrix(y, y_pred)
print(conf_mat) 
acc=(conf_mat[0,0]+conf_mat[1,1])/(conf_mat[0,0]+conf_mat[1,1]+conf_mat[0,1]+conf_mat[1,0])
print("Mean 5-Fold accuracy of the logistic regression is: {}".format(acc))
results.append(acc)

#Decision tree pca 
tree = DecisionTreeClassifier()
y_pred = cross_val_predict(tree, x, y, cv=5)
conf_mat = confusion_matrix(y, y_pred)
print(conf_mat) 
acc=(conf_mat[0,0]+conf_mat[1,1])/(conf_mat[0,0]+conf_mat[1,1]+conf_mat[0,1]+conf_mat[1,0])
print("Mean 5-Fold accuracy of the decision tree is: {}".format(acc))
results.append(acc)

#Support vector machine pca
svm = svm.SVC(kernel='linear') 
y_pred = cross_val_predict(svm, x, y, cv=5)
conf_mat = confusion_matrix(y, y_pred)
print(conf_mat) 
acc=(conf_mat[0,0]+conf_mat[1,1])/(conf_mat[0,0]+conf_mat[1,1]+conf_mat[0,1]+conf_mat[1,0])
print("Mean 5-Fold accuracy of the support vector machine is: {}".format(acc))
results.append(acc)

"""##Models with feature selection"""

#Correlation matrix
corrmat = df1.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(df1[top_corr_features].corr(),annot=True,cmap="RdYlGn")

#Correlation with output variable
cor_target = abs(df1.corr()["Erasmus"])
#Selecting highly correlated features--> correlation higher than 20%
relevant_features = cor_target[cor_target>0.2]
relevant_features

#Adding the current grades mean column (even if correlation <0.20) because logically and practically significant  
cols = [col for col in df1.columns if col in relevant_features]
df_s = df1[cols]
grades = df1["current_grades_mean"]
df_s = df_s.join(grades)
df_s

#Separating target and predictors
y_s = df_s['Erasmus'].values
x_s = df_s.drop(df_s.columns[[1]], axis=1)
scaler = StandardScaler()
scaler.fit(x_s)
x_s = scaler.transform(x_s)

#Logistic regression feature selection
log_regr = LogisticRegression()
y_pred = cross_val_predict(log_regr, x_s, y_s, cv=5)
conf_mat = confusion_matrix(y_s, y_pred)
print(conf_mat) 
acc=(conf_mat[0,0]+conf_mat[1,1])/(conf_mat[0,0]+conf_mat[1,1]+conf_mat[0,1]+conf_mat[1,0])
print("Mean 5-Fold accuracy of the logistic regression is: {}".format(acc))
results.append(acc)

#Decision tree feature selection 
tree = DecisionTreeClassifier()
y_pred = cross_val_predict(tree, x_s, y_s, cv=5)
conf_mat = confusion_matrix(y_s, y_pred)
print(conf_mat) 
acc=(conf_mat[0,0]+conf_mat[1,1])/(conf_mat[0,0]+conf_mat[1,1]+conf_mat[0,1]+conf_mat[1,0])
print("Mean 5-Fold accuracy of the decision tree is: {}".format(acc))
results.append(acc)

#Support vector machine feature selection 
svm = SVC(kernel='linear') 
y_pred = cross_val_predict(svm, x_s, y_s, cv=5)
conf_mat = confusion_matrix(y_s, y_pred)
print(conf_mat) 
acc=(conf_mat[0,0]+conf_mat[1,1])/(conf_mat[0,0]+conf_mat[1,1]+conf_mat[0,1]+conf_mat[1,0])
print("Mean 5-Fold accuracy of the support vector machine is: {}".format(acc))
results.append(acc)

#Final graph for the comparison of the results of the different models 
names=['baseline_LR','LR_PCA','DT_PCA','SVM_PCA','LR','DT','SVM']
fig = plt.figure(figsize=(7,7))
fig.suptitle('Models Comparison')
ax = fig.add_subplot(111)
plt.scatter(names,results)
plt.xlabel('Models')
plt.ylabel('Accuracy')
ax.set_xticklabels(names)
ax.set_ylim([0, 1])
plt.show()